# conflicting_datetime_formats

#  This transform tests three different date time formats and selects the first match
[conflicting_datetime_formats-test_try_formats]
INGEST_EVAL= _time=case(isnotnull(strptime(_raw, "%c")), strptime(_raw, "%c"), isnotnull(strptime(_raw, "%H:%M:%S %y-%m-%d")),strptime(_raw, "%H:%M:%S %y-%m-%d"), isnotnull(strptime(_raw, "%Y-%m-%d %H:%M:%S")), strptime(_raw, "%Y-%m-%d %H:%M:%S"))

# use regex replace to pop out the date form the source, append on the first 10 chars from _raw and then run through strftime.
# if the eval fails to execute, CURRENT time will be kept
[compound_datetimes-join_and_parse]
INGEST_EVAL= _time=strptime(replace(source,".*/(20\d\d\-\d\d\-\d\d)\.log","\1").substr(_raw,0,10),"%Y-%m-%d%H:%M:%S"), my_date:=null() 

# add the length of the _raw string to the event as a hidden field, this needs to be the last transform so we don't change the
# length of _raw again once the value has been computed
[add_event_length-get_raw_length]
INGEST_EVAL= _event_length=len(_raw)

# To drop a field from _meta we need to overwrite any previous value using the := assignment option
[drop_indexed_fields-drop_useless_columns]
INGEST_EVAL= time:=null(), repeated_field:=null(), random_nonsense:=null(), long_payload:=null()

# To drop a field from _meta we need to overwrite any previous value using the := assignment option
[shared-drop_useless_time_fields]
INGEST_EVAL= timestartpos:=null(), timeendpos:=null(), date_second:=null(), date_hour:=null(), date_minute:=null(), date_year:=null(), date_month:=null(), date_mday:=null(),  date_wday:=null(), date_zone:=null()

# This transform routes data to the null queue, affectively deleting it before it is written to the index
[shared_drop_event]
INGEST_EVAL = queue="nullQueue"

# the header field form a Splunk CSV export starts with the first row being named after the header _raw. We want to drop these
[load_into_indexes-drop_header]
INGEST_EVAL = queue=if(_raw="\"_raw\"","nullQueue", queue)

# We use REGEX to pop out the values for index, host, sourcetype & source, we then write them to tempory variables in _meta.
# We assume that % is not found in the primary keys to optimize the REGEX
[load_into_indexes-extract_metadata_copy_to_meta]
SOURCE_KEY=_raw
WRITE_META = true
REGEX = ^"\d+(?:\.\d+)?%%%([^%]+)%%%([^%]+)%%%([^%]+)%%%([^%]+)%%%
FORMAT = my_index::"$1" my_host::"$2" my_source::"$3" my_sourcetype::"$4"

# copy the temporary user defined fields into the primary metadata locations and then delete the temporary fields
[load_into_indexes-reassign_meta_to_metadata]
INGEST_EVAL = host:=my_host, source:=my_source, index:=my_index, sourcetype:=my_sourcetype, my_host:=null(), my_source=null(), my_index:=null(), my_sourcetype:=null()

# extract the _raw field from the protocol and write back to _raw
[load_into_indexes-remove_metadata_from_raw]
INGEST_EVAL = _raw=replace(_raw, "^[^%]+%%%(?:[^%]+)%%%(?:[^%]+)%%%(?:[^%]+)%%%(?:[^%]+)%%%(.*)","\1")

# debug function that copies the contents of the _meta field into the _raw field so we can see what is being routed to indexing
[shared-copy_meta_to_raw]
SOURCE_KEY = _meta
DEST_KEY = _raw
REGEX = (.*)
FORMAT = $1


# this regex finds unquote quoted attribute value pairs, ie the form a=b, and appends them to _meta
[auto_extract_indexed_fields-univeral]
SOURCE_KEY = _raw
REGEX = \s([a-zA-Z][a-zA-Z0-9_-]+)=(?:"([^"]+)"|'([^']+)'|([^\s"',]+))
REPEAT_MATCH=true
FORMAT = $1::"$2$3$4"
WRITE_META = true

# this transform randomly assigns each event to one of the two output groups by appending a 0 or 1 to the named output group
[split_forwarding-randomize_output]
INGEST_EVAL = _TCP_ROUTING="split_forwarding_".random()%2

# create a copy of the license log files, we leave the orginal event to be written into the _internal index as normal
[metricify_license-clone]
REGEX = .
CLONE_SOURCETYPE = metricify_license
FORMAT = $0
DEST_KEY = _raw

# We use a REGEX transformation to lift the fields and then write them to indexed fields, the value for b becomes _value which is the 
# metric value, the remaining fields will become dimensions, we hard code the metric name to become license_usage
# we create two mutally exclusive capture groups for each field that may or may not be quoted with strings, and assign both to the indexed field 
[metricify_license-extract_fields]
SOURCE_KEY = _raw
REGEX = type=(\S+)\ss=(?:"([^"]*)"|(\S*))\s+st=(?:"([^"]+)"|(\S+))\s+h=(?:"([^"]+)"|(\S+))\so="[^"]*"\sidx=(?:"([^"]+)"|(\S+))\si="([^"]+)"\spool="([^"]+)"\sb=(\d+)\spoolsz=(\d+)
FORMAT = metric_name::"license_usage" type::"$1" from_source::"$2$3" from_sourcetype::"$4$5" from_host::"$6$7" into_index::"$8$9" on_indexer::"$10" allocated_to_pool::"$11" _value::"$12" pool_size::"$13"
WRITE_META = true

# We keep data when type=usage, copy the metadata into the proper fields, drop the immediate indexed fields, and then route the metrics data into the _metrics index
[metricify_license-reassign_meta_fields_and_route_to_index]
INGEST_EVAL = queue=if(type="Usage", queue, "nullQueue"), host=from_host, sourcetype=from_sourcetype, source=from_source,  type:=null(), from_host:=null(), from_sourcetype:=null(), from_source:=null(), index:="_metrics"

# Use REGEX to extract the values for component, log_level, thread_id and thread_name as indexed fields
[enrich_splunkd_component_level_thread-extract_log_level_etc]
SOURCE_KEY = _raw
WRITE_META = true
REGEX = ^\d\d\-\d\d-\d{4}\s\d\d:\d\d\:\d\d\.\d{0,6}\s[+\-]\d{4}\s(INFO|DEBUG|ERROR|WARN|DEBUG)\s+(\S+)\s(?:\[(\d+)\s(\S+)\])?
FORMAT = log_level::$1 component::$2 thread_id::$3 thread_name::$4 

# the values for thread name and thread_id are optional which will produce empty indexed fields, we use this transform to remove them
[enrich_splunkd_component_level_thread-drop_null_thread_info]
INGEST_EVAL = thread_id:=if(thread_id="",null(),thread_id), thread_name:=if(thread_name="",null(),thread_name)

# lets extract the parameters from splunkd_access.log and write them into indexed fields
[enrich_splunkd_access_log-decompose_url]
SOURCE_KEY = _raw
WRITE_META = true
REGEX= (\d{0,3}\.\d{0,3}\.\d{0,3}\.\d{0,3})\s-\s(\S*)\s\[[^\]]+\]\s"(GET|POST)\s(\S+)\s([^"]+)"\s(\d+)\s(\d+)\s-\s\-\s-\s(\d+)ms
FORMAT= source_ip::"$1" user::"$2" http_type::"$3" full_url::"$4" http_version::"$5" http_return_code::"$6" http_return_code_2::"$7" response_time_ms::"$8"

# We cannot target _meta with a REGEX transform, so we will copy the URL into the host field
# This means we must stash the existing host field as an indexed field until we are done
# We then apply the split function to the full URL to break around the ? REST delimiter 
# We copy the MV paramaters field into the host field so we can use REGEX repeat match
[enrich_splunkd_access_log-move_url_to_host]
INGEST_EVAL = my_host:=host, base_url:=mvindex(split($full_url$,"?"),0), base_url:=if(isnull(base_url),full_url,base_url), host=mvindex(split($full_url$,"?"),1) 

# we have copied the URL paramaters to host, lets extact this and extract the paramaters one by one creating indexed fields
[enrich_splunkd_access_log-extract_url_parameters_from_host_to_meta]
SOURCE_KEY = MetaData:Host
WRITE_META = true
REGEX = ([^=&:]+)=([^&]+)
FORMAT = rest_param_$1::"$2"
REPEAT_MATCH = true

# copy back my_host to host and drop my_host
# we don't want to index the full url as it is will bloat tsidx,  we don't need my_host 
[enrich_splunkd_access_log-meta_copy_back]
INGEST_EVAL=host:=my_host, my_host:=null(), full_url:=null()

# we are going clone the data, so we can reparse it all dropping the original
[shard_data_with_splitbyindexkeys-clone]
REGEX = .
CLONE_SOURCETYPE = shard_data_with_splitbyindexkeys
FORMAT = $0
DEST_KEY = _raw

# we use REGEX to extract the meta data encoded in the host name
[shard_data_with_splitbyindexkeys-extract_metadata_from_host]
SOURCE_KEY = MetaData:Host
WRITE_META = true
REGEX = ^(?:host::)?([^\-]+)-([^\.]+)+\.([^\.]+)\.(.+)$
FORMAT = role::"$1" instance::"$2" stack::"$3" domain::"$4"

# we want to write the component name into to sourcetype field, and stack name into the source field, if the component isn't extracted we send to detritus.
[shard_data_with_splitbyindexkeys-move_meta_data]
INGEST_EVAL =  orig_source=source, orig_sourcetype=sourcetype, source=stack, sourcetype=component, index=if(isnull(component), "detritus", index)

# we create 
[mask_and_clone-clone]
SOURCE_KEY = Meta:Host
DEST_KEY = Meta:Host
REGEX = (.)
FORMAT = %0
CLONE_SOURCETYPE = mask_and_clone-clone

[mask_and_clone-route_to_index]
INGEST_EVAL = index="ingest_eval_examples_2"

[mask_data_and_map-extract_password]
SOURCE_KEY = _raw
WRITE_META = true
REGEX = password="(.*)"$
FORMAT = password::"$1"

[mask_data_and_map-extract_email]
SOURCE_KEY = _raw
WRITE_META = true
REGEX = email_address=(\S+@\S+)
FORMAT = email_address::"$1"

[mask_data_and_map-clone]
SOURCE_KEY = Meta:Host
DEST_KEY = Meta:Host
REGEX = (.)
FORMAT = %0
CLONE_SOURCETYPE = mask_data_and_map-clone

[mask_data_and_map-encode_email_and_password]
INGEST_EVAL =  email_address_encoded=substr(sha1(email_address),0,10), password_encoded=substr(sha1(password),0,10)

[mask_data_and_map-apply_hashes]
INGEST_EVAL = _raw=replace(_raw, "email_address=\S+", "email_address=".email_address_encoded), _raw=replace(_raw, "password=\".*\"$","password=\"".password_encoded."\""), email_address:=null(), password :=null(), password_encoded:=null(), email_address_encoded:=null()

[mask_data_and_map-create_map]
INGEST_EVAL = _raw=".", email_address_decoded:=email_address, email_address:=email_address_encoded, password_decoded:=password, password:=password_encoded, password_encoded:=null(), email_address_encoded:=null