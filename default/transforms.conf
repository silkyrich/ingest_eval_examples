# conflicting_datetime_formats

#  This transform tests three different date time formats and selects the first match
[conflicting_datetime_formats-test_try_formats]
INGEST_EVAL= _time=case(isnotnull(strptime(_raw, "%c")), strptime(_raw, "%c"), isnotnull(strptime(_raw, "%H:%M:%S %y-%m-%d")),strptime(_raw, "%H:%M:%S %y-%m-%d"), isnotnull(strptime(_raw, "%Y-%m-%d %H:%M:%S")), strptime(_raw, "%Y-%m-%d %H:%M:%S"))

# use regex replace to pop out the date form the source, append on the first 10 chars from _raw and then run through strftime.
# if the eval fails to execute, CURRENT time will be kept
[compound_datetimes-join_and_parse]
INGEST_EVAL= _time=strptime(replace(source,".*/(20\d\d\-\d\d\-\d\d)\.log","\1").substr(_raw,0,10),"%Y-%m-%d%H:%M:%S"), my_date:=null() 

# add the length of the _raw string to the event as a hidden field, this needs to be the last transform so we don't change the
# length of _raw again once the value has been computed
[add_event_length-get_raw_length]
INGEST_EVAL= _event_length=len(_raw)

# To drop a field from _meta we need to overwrite any previous value using the := assignment option
[drop_indexed_fields-drop_useless_columns]
INGEST_EVAL= time:=null(), repeated_field:=null(), random_nonsense:=null(), long_payload:=null()

# To drop a field from _meta we need to overwrite any previous value using the := assignment option
[shared-drop_useless_time_fields]
INGEST_EVAL= timestartpos:=null(), timeendpos:=null(), date_second:=null(), date_hour:=null(), date_minute:=null(), date_year:=null(), date_month:=null(), date_mday:=null(),  date_wday:=null(), date_zone:=null()

# This transform routes data to the null queue, affectively deleting it before it is written to the index
[shared_drop_event]
INGEST_EVAL = queue="nullQueue"

# the header field form a Splunk CSV export starts with the first row being named after the header _raw. We want to drop these
[load_into_indexes-drop_header]
INGEST_EVAL = queue=if(_raw="\"_raw\"","nullQueue", queue)

# We use REGEX to pop out the values for index, host, sourcetype & source, we then write them to tempory variables in _meta.
# We assume that % is not found in the primary keys to optimize the REGEX
[load_into_indexes-extract_metadata_copy_to_meta]
SOURCE_KEY=_raw
WRITE_META = true
REGEX = ^"\d+(?:\.\d+)?%%%([^%]+)%%%([^%]+)%%%([^%]+)%%%([^%]+)%%%
FORMAT = my_index::"$1" my_host::"$2" my_source::"$3" my_sourcetype::"$4"

# copy the temporary user defined fields into the primary metadata locations and then delete the temporary fields
[load_into_indexes-reassign_meta_to_metadata]
INGEST_EVAL = host:=my_host, source:=my_source, index:=my_index, sourcetype:=my_sourcetype, my_host:=null(), my_source=null(), my_index:=null(), my_sourcetype:=null()

# extract the _raw field from the protocol and write back to _raw
[load_into_indexes-remove_metadata_from_raw]
INGEST_EVAL = _raw=replace(_raw, "^[^%]+%%%(?:[^%]+)%%%(?:[^%]+)%%%(?:[^%]+)%%%(?:[^%]+)%%%(.*)","\1")

# debug function that copies the contents of the _meta field into the _raw field so we can see what is being routed to indexing
[shared-copy_meta_to_raw]
SOURCE_KEY = _meta
DEST_KEY = _raw
REGEX = (.*)
FORMAT = $1


# this regex finds unquote quoted attribute value pairs, ie the form a=b, and appends them to _meta
[auto_extract_indexed_fields-univeral]
SOURCE_KEY = _raw
REGEX = \s([a-zA-Z][a-zA-Z0-9_-]+)=(?:"([^"]+)"|'([^']+)'|([^\s"',]+))
REPEAT_MATCH=true
FORMAT = $1::"$2$3$4"
WRITE_META = true

# this transform randomly assigns each event to one of the two output groups by appending a 0 or 1 to the named output group
[split_forwarding-randomize_output]
INGEST_EVAL = _TCP_ROUTING="split_forwarding_".random()%2

# create a copy of the license log files, we leave the orginal event to be written into the _internal index as normal
[metricify_license-clone]
REGEX = .
CLONE_SOURCETYPE = metricify_license
FORMAT = $0
DEST_KEY = _raw

# We use a REGEX transformation to lift the fields and then write them to indexed fields, the value for b becomes _value which is the 
# metric value, the remaining fields will become dimensions, we hard code the metric name to become license_usage
# we create two mutally exclusive capture groups for each field that may or may not be quoted with strings, and assign both to the indexed field 
[metricify_license-extract_fields]
SOURCE_KEY = _raw
REGEX = type=(\S+)\ss=(?:"([^"]*)"|(\S*))\s+st=(?:"([^"]+)"|(\S+))\s+h=(?:"([^"]+)"|(\S+))\so="[^"]*"\sidx=(?:"([^"]+)"|(\S+))\si="([^"]+)"\spool="([^"]+)"\sb=(\d+)\spoolsz=(\d+)
FORMAT = metric_name::"license_usage" type::"$1" from_source::"$2$3" from_sourcetype::"$4$5" from_host::"$6$7" into_index::"$8$9" on_indexer::"$10" allocated_to_pool::"$11" _value::"$12" pool_size::"$13"
WRITE_META = true

# We keep data when type=usage, copy the metadata into the proper fields, drop the immediate indexed fields, and then route the metrics data into the _metrics index
[metricify_license-reassign_meta_fields_and_route_to_index]
INGEST_EVAL = queue=if(type="Usage", queue, "nullQueue"), host=from_host, sourcetype=from_sourcetype, source=from_source,  type:=null(), from_host:=null(), from_sourcetype:=null(), from_source:=null(), index:="_metrics"

# Use REGEX to extract the values for component, log_level, thread_id and thread_name as indexed fields
[enrich_splunkd_component_level_thread-extract_log_level_etc]
SOURCE_KEY = _raw
WRITE_META = true
REGEX = ^\d\d\-\d\d-\d{4}\s\d\d:\d\d\:\d\d\.\d{0,6}\s[+\-]\d{4}\s(INFO|DEBUG|ERROR|WARN|DEBUG)\s+(\S+)\s(?:\[(\d+)\s(\S+)\])?
FORMAT = log_level::$1 component::$2 thread_id::$3 thread_name::$4 

# the values for thread name and thread_id are optional which will produce empty indexed fields, we use this transform to remove them
[enrich_splunkd_component_level_thread-drop_null_thread_info]
INGEST_EVAL = thread_id:=if(thread_id="",null(),thread_id), thread_name:=if(thread_name="",null(),thread_name)

# lets extract the parameters from splunkd_access.log and write them into indexed fields
[enrich_splunkd_access_log-decompose_url]
SOURCE_KEY = _raw
WRITE_META = true
REGEX= (\d{0,3}\.\d{0,3}\.\d{0,3}\.\d{0,3})\s-\s(\S*)\s\[[^\]]+\]\s"(GET|POST)\s(\S+)\s([^"]+)"\s(\d+)\s(\d+)\s-\s\-\s-\s(\d+)ms
FORMAT= source_ip::"$1" http_user::"$2" http_type::"$3" full_url::"$4" http_version::"$5" http_return_code::"$6" http_return_code_2::"$7" response_time_ms::"$8"

# We cannot target _meta with a REGEX transform, so we will copy the URL into the host field
# This means we must stash the existing host field as an indexed field until we are done
# We then apply the split function to the full URL to break around the ? REST delimiter 
# We copy the MV paramaters field into the host field so we can use REGEX repeat match
[enrich_splunkd_access_log-move_url_to_host]
INGEST_EVAL = my_host:=host, base_url:=mvindex(split($full_url$,"?"),0), base_url:=if(isnull(base_url),full_url,base_url), host=mvindex(split($full_url$,"?"),1) 

# we have copied the URL paramaters to host, lets extact this and extract the paramaters one by one creating indexed fields
[enrich_splunkd_access_log-extract_url_parameters_from_host_to_meta]
SOURCE_KEY = MetaData:Host
WRITE_META = true
REGEX = ([^=&:]+)=([^&]+)
FORMAT = rest_param_$1::"$2"
REPEAT_MATCH = true

# copy back my_host to host and drop my_host
# we don't want to index the full url as it is will bloat tsidx,  we don't need my_host 
[enrich_splunkd_access_log-meta_copy_back]
INGEST_EVAL=host:=my_host, my_host:=null(), full_url:=null()

# we are going clone the data, so we can reparse it all dropping the original
[shard_data_with_splitbyindexkeys-clone]
REGEX = .
CLONE_SOURCETYPE = shard_data_with_splitbyindexkeys
FORMAT = $0
DEST_KEY = _raw

# we use REGEX to extract the meta data encoded in the host name
[shard_data_with_splitbyindexkeys-extract_metadata_from_host]
SOURCE_KEY = MetaData:Host
WRITE_META = true
REGEX = ^(?:host::)?([^\-]+)-([^\.]+)+\.([^\.]+)\.(.+)$
FORMAT = role::"$1" instance::"$2" stack::"$3" domain::"$4"

# we want to write the component name into to sourcetype field, and stack name into the source field, if the component isn't extracted we send to detritus.
[shard_data_with_splitbyindexkeys-move_meta_data]
INGEST_EVAL =  orig_source=source, orig_sourcetype=sourcetype, source=stack, sourcetype=component, index=if(isnull(component), "detritus", index)

# we create a clone
[mask_and_clone-clone]
SOURCE_KEY = Meta:Host
DEST_KEY = Meta:Host
REGEX = (.)
FORMAT = %0
CLONE_SOURCETYPE = mask_and_clone-clone

# routes the cloned data to a different index
[shared-route_to_ingest_eval_examples_2]
INGEST_EVAL = index="ingest_eval_examples_2"

# This REGEX extracts the password string from the event. This is used instead of the INGEST_EVAL replace function because it will use compilied REGEX
# Using REGEX on a password is interesting as we have no idea what is in the password, it could contain whitespace or any other charactor
# For the example event we know it is the last field and we are able to match on $ for the end of the string
[mask_data_and_map-extract_password]
SOURCE_KEY = _raw
WRITE_META = true
REGEX = password="(.*)"$
FORMAT = password::"$1"

# This REGEX extracts the email address, we could merge this REGEX with the extract password if we wanted
# Again we prefer REGEX over replace() for the assumed performance boost
[mask_data_and_map-extract_email]
SOURCE_KEY = _raw
WRITE_META = true
REGEX = email_address=(\S+@\S+)
FORMAT = email_address::"$1"

# This REGEX transform CLONES the data, for some reason it has to do a REGEX match and replace, this is cheap one as host is short and we only match a single charactor
[mask_data_and_map-clone]
SOURCE_KEY = Meta:Host
DEST_KEY = Meta:Host
REGEX = (.)
FORMAT = %0
CLONE_SOURCETYPE = mask_data_and_map-clone

# We want to get an encoded username and password so that we can track user names annoymously, however we don't want users to have the ability to see which users share the same password
# To stop this, we include the user name in the hash with the password, this allows us to see if the password has been changed, but much harder to work out if passwords are shared
# We perform this operation before we clone, so that the heavy SHA1 computation is used once. Also to make the output easier to read we clip off the first 10 charactors rather
# than insert a long string into the event
[mask_data_and_map-encode_email_and_password]
INGEST_EVAL =  email_address_encoded=substr(sha1(email_address),0,10), password_encoded=substr(sha1(email_address.password),0,10)

# We take the original clone event and use replace to swap out the email address and password for the encrypted versions, then we drop the encoded password etc.
# We also drop the password and user name, although this isn't necessary and it might be more useful to keep as indexed fields
[mask_data_and_map-apply_hashes]
INGEST_EVAL = _raw=replace(_raw, "email_address=\S+", "email_address=".email_address_encoded), _raw=replace(_raw, "password=\".*\"$","password=\"".password_encoded."\""), email_address:=null(), password :=null(), password_encoded:=null(), email_address_encoded:=null()

# The cloned event is going to be turned into a map. We set the _raw string to be . to save on license costs and shuffle around the field names to make it easier to use at search time
[mask_data_and_map-create_map]
INGEST_EVAL = _raw=".", email_address_decoded:=email_address, email_address:=email_address_encoded, password_decoded:=password, password:=password_encoded, password_encoded:=null(), email_address_encoded:=null

# rename the user defined host into a meeting_host
[name_clash_rename_host]
INGEST_EVAL = meeting_host=$field:host$, $field:host$:=null()