# This sourcetype is for the conflicting timestamps usecase, it calls a transform to extract the 
# datetime format via INGEST_EVAL and the strptime function
[conflicting_datetime_formats]
DATETIME_CONFIG = CURRENT
TRANSFORMS-extract_date = conflicting_datetime_formats-test_try_formats

# This sourcetype is for the compound datetimes examples where the file name encodes the date stamp
# but the timestamp is per log line
[compound_datetimes]
DATETIME_CONFIG = CURRENT
TRANSFORMS-get-date = compound_datetimes-join_and_parse
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)

# This configuration is applied to all sourcetypes 
[default]
# Adds an indexed field with len of the event, useful for using with tstats to sum up all ingested data from any source very quickly
TRANSFORM-all-data = add_event_length-get_raw_length
# Lets turn off the annoying timefields across the entire ingestion pipeline
ADD_EXTRA_TIME_FIELDS = subseconds

# this is an example for removing unwanted indexed fields from a CSV file
[drop_indexed_fields]
# The timestamp is found in the time column, we need to remove this column
TIMESTAMP_FIELDS=time
INDEXED_EXTRACTIONS = CSV
# These transforms remove the useless columns, and also the useless time fields
TRANSFORMS-drop_fields = drop_indexed_fields-drop_useless_columns, shared-drop_useless_time_fields
# Lift the removed columns via search time exraction (only works in the app)
EXTRACT-removed-columns= [^,]+,[^,]+,[^,]+,(?<random_nonsense>[^,]+),(?<long_payload>[^,]+)

# this is an example for importing data from an external splunk instance
[load_into_indexes]
# time is stored in epoch at the start of each line
TIME_FORMAT = %s.%3Q
TIME_PREFIX = ^
# the transform required to drop the header extract the metafields and copy to the correct fields
TRANSFORMS-extract-metadata = load_into_indexes-drop_header, load_into_indexes-extract_metadata_copy_to_meta, load_into_indexes-reassign_meta_to_metadata, load_into_indexes-remove_metadata_from_raw
# Splunk uses double quotes to escape quotes, we want to remove this before we start extracting the fields
SEDCMD-strip_double_quotes= s/""/"/g
# The solution supports multiline events
LINE_BREAKER=(\^\^\^END\^\^\^"\n)
SHOULD_LINEMERGE=false

# This sourcetype is an example for how we can use REPEAT_MATCH and regex to automatically extract fields from log files
[auto_extract_indexed_fields]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
# because we are creating indexed fields we can disable the major breakers
SEGMENTATION = search
TRANSFORMS-extract_indexed_fields=auto_extract_indexed_fields-univeral

# this transform applies a REGEX transform to extract the log_level, the component and thread name and if is it there
[enrich_splunkd_component_level_thread]
TIME_PREFIX = ^
TIME_FORMAT = %m-%d-%Y %H:%M:%S.%l %z
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
TRANSFORMS-enrich-splunkd-log_level = enrich_splunkd_component_level_thread-extract_log_level_etc, enrich_splunkd_component_level_thread-drop_null_thread_info

# This sourcetype sends data out via TCP out
[split_forwarding]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
rename = split_forwarding_sendout
TRANSFORMS-split_forwarding=split_forwarding-randomize_output

# license usage data is under the splunkd sourcetype, so we need to match on the source file to be more selective 
# The transform uses CLONE_SOURCETYPE to create a copy of the event for further processing
[source::.../var/log/splunk/license_usage.log(.\d+)?]
TRANSFORMS-clone_license = metricify_license-clone

# These transforms convert the event into a metrics compliant one, and steer to the _metrics index 
[metricify_license]
TRANSFORMS-clone_and_convert = metricify_license-extract_fields, shared-drop_useless_time_fields, metricify_license-reassign_meta_fields_and_route_to_index

# During loading we would like to use REGEX to extract the URL metadata from splunkd_access.log 
[enrich_splunkd_access_log]
TIME_PREFIX = \[
TIME_FORMAT = %d/%b/%Y:%H:%M:%S.%f %z
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
TRANSFORMS-enrich_splunkd_access_log = enrich_splunkd_access_log-decompose_url, enrich_splunkd_access_log-move_host_to_meta, enrich_splunkd_access_log-move_url_to_host, enrich_splunkd_access_log-extract_url_parameters_from_host_to_meta, enrich_splunkd_access_log-meta_copy_back, shared-drop_useless_time_fields 
# we are fully decomposing the URL we don't need MAJOR breakers
SEGMENTATION=search

# We are going to combine a few of the examples together to get our data into the index
# We want to load data from the CSV and write it to a new source type dropping the orginal
[shard_data_with_splitbyindexkeys_import]
TIME_FORMAT = %s.%3Q
TIME_PREFIX = ^
# the transform required to drop the header extract the metafields and copy to the correct fields
TRANSFORMS-shard_data_with_splitbyindexkeys_import = load_into_indexes-drop_header, load_into_indexes-extract_metadata_copy_to_meta, load_into_indexes-reassign_meta_to_metadata, load_into_indexes-remove_metadata_from_raw, shard_data_with_splitbyindexkeys-clone, shared_drop_event
# Splunk uses double quotes to escape quotes, we want to remove this before we start extracting the fields
SEDCMD-strip_double_quotes= s/""/"/g
# The solution supports multiline events
LINE_BREAKER=(\^\^\^END\^\^\^"\n)
SHOULD_LINEMERGE=false

# This is the process for extracting the meta data from a splunkd event and writting it to the sharded index
[shard_data_with_splitbyindexkeys]
TRANSFORMS-shard_data_with_splitbyindexkeys = enrich_splunkd_component_level_thread-extract_log_level_etc, enrich_splunkd_component_level_thread-drop_null_thread_info, shard_data_with_splitbyindexkeys-extract_metadata_from_host, shard_data_with_splitbyindexkeys-move_meta_data
# we are fully decomposing the URL we don't need MAJOR breakers
SEGMENTATION=search

# Apply the enrichment to all splunkd events
#[splunkd]
#TRANSFORMS-enrich-splunkd-log_level = auto_extract_indexed_fields-univeral, enrich_splunkd_component_level_thread-extract_log_level_etc, enrich_splunkd_component_level_thread-drop_null_thread_info

# This sourcetype is cloned and the clone has masking applied, the orignal remains untouched and is routed to the 'secure' index
[mask_and_clone]
TIME_PREFIX = ^â€º
TIME_FORMAT = %H:%M:%S %y-%m-%d
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+) 
# this transform will create a new event with a tempoary sourcetype
TRANSFORMS-apply-masking=mask_and_clone-clone

# we intercept the tempory sourcetype, apply some sed transforms, rename the sourcetype to the original and route to another 'insecure' index
[mask_and_clone-clone]
# because we have created a new event we can apply a SED transform to the newly created event
# we could use the INGEST_EVAL replace function to do this, but using SED is slighly more elegent and is likely use a faster engine and the replace function.
# Note that we have specfied two SED commands separated by a space
SEDCMD-mask = s/email_address=(\S\S)\S*(.)@(.)\S*(\S\S\S)\s/email_address=\1..\2@\3..\4 /g s/password=\S+/password=#######/g
# we need to route this new event to another index to prove our solution
TRANSFORMS-route-to-index = shared-drop_useless_time_fields, shared-route_to_ingest_eval_examples_2
# we can set the sourcetype back to the orginal
rename = mask_and_clone

# This is a much more complex masking that creates an cloned event that is transformed into a "map"
[mask_data_and_map]
TIME_PREFIX = ^
TIME_FORMAT = %H:%M:%S %y-%m-%d
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
TRANSFORMS-apply-masking=shared-drop_useless_time_fields, mask_data_and_map-extract_email, mask_data_and_map-extract_password, mask_data_and_map-encode_email_and_password, mask_data_and_map-clone, mask_data_and_map-apply_hashes

# Take the cloned event, transform to a map and send to a different index
[mask_data_and_map-clone]
TRANSFORMS-create_map = shared-route_to_ingest_eval_examples_2, mask_data_and_map-create_map
rename=mask_data_and_map-map

# This demonstrates how to fix clashing names found when using INDEXED_EXTRACTIONS
[name_clash]
INDEXED_EXTRACTIONS = JSON
TIMESTAMP_FIELDS = start_time
TIME_FORMAT = %Y-%m-%dT%T
TRANSFORMS-rename_host = name_clash_rename_host

# json docker messages embedd the log line as a field
[json_docker]
# We will use the JSON parser to decompose the JSON structure into indexed fields
INDEXED_EXTRACTIONS = JSON
# The value for time is store in the time field
TIMESTAMP_FIELDS = time
TIME_FORMAT = %Y-%m-%dT%H:%M:%S.%QZ
# Use INGEST_EVAL to reassign the fields and replace the _raw string
TRANSFORMS-json_docker = json_docker_reassign_raw_drop_time
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)

