{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This workbook generates sample data for the app and writes to the sample directory. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Set up the global variables for the script\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import uuid\n",
    "import string\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "data_directory=\"appserver/static/sample_data\"\n",
    "\n",
    "\"\"\" \n",
    "For events we have selected a selection of Splunk T shirt sloans. This list was obtained by searching the web, it is not a definitive list and I suspect many were never printed :-)\n",
    "\"\"\"\n",
    "log_lines = open(data_directory+\"/splunk_slogans.txt\",\"r\").read().splitlines()\n",
    "\n",
    "\"\"\"\n",
    "The script generates events randomly over a time range, by default this goes back 5 days and generates a 1000 events each time.\n",
    "\"\"\"\n",
    "date_range_days=2\n",
    "sample_readings=1000\n",
    "\n",
    "# Get an random array of datetime objects going backwards in time, sorted oldest first\n",
    "def get_dates(sample_readings : int, max_days_ago : int) : \n",
    "    datetimes = []\n",
    "    for i in range(0,sample_readings): \n",
    "        random_seconds = random.randrange(1, max_days_ago*24*60*60)\n",
    "        my_timedelta=datetime.timedelta(seconds=-random_seconds, milliseconds=random.randint(0,9999))\n",
    "        my_datetime=datetime.datetime.now()+my_timedelta\n",
    "        datetimes.append(my_datetime)\n",
    "    # Sort the dates into reverse chroniclogical order as they would appear in a log file\n",
    "    datetimes.sort(reverse=False)\n",
    "    return datetimes\n",
    "\n",
    "\"\"\"\n",
    "This generates a list of events where the time stamps switches between 3 different timestamps.\n",
    "\n",
    "This is a common problem in badly designed Splunk instances. People open a TCP port and then fire all sorts of different data in there. \n",
    "\n",
    "Ideally we would create multiple sourcetypes and then assign a TCP port for each sourcetype. However the example shows you how to patch the problem during ingestion.\n",
    "\"\"\"\n",
    "def conflicting_datetime_formats(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # our three different date time formats\n",
    "    datetime_format = [\"%Y-%m-%d %H:%M:%S\", \"%H:%M:%S %y-%m-%d\", \"%c\"]\n",
    "\n",
    "    # create out output file\n",
    "    mutliplexed_datetime_formats = open(data_directory+\"/conflicting_datetime_formats/mutliplexed_datetime_formats.log\",\"w\")\n",
    "\n",
    "    # iterate through the list of date timesn and write out to disk\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # select a timeformat at random and use it\n",
    "        time=my_datetime.strftime(random.choice(datetime_format))\n",
    "        # pick a random log line to use\n",
    "        message=random.choice(log_lines)\n",
    "        # write out the log file\n",
    "        mutliplexed_datetime_formats.write(time+\" \"+message+\"\\n\")\n",
    "\n",
    "    # close and flush the file\n",
    "    mutliplexed_datetime_formats.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script generates events where the date is embedded in the file name, but the timestamp is per event im the contents of the file.\n",
    "\n",
    "We are going to create a map of dates to times, so that we can itterate through each day, create a file and fill with events for that day\n",
    "\n",
    "To work around any weird rounding errors due to timezones we will generate the day, and the seconds separately\n",
    "\"\"\"\n",
    "def compound_datetimes(sample_readings : int, max_days_ago : int) : \n",
    "    # create our map for the date to timings mapping\n",
    "    date_map = {}\n",
    "\n",
    "    path = data_directory+\"/compound_datetimes/\"\n",
    "\n",
    "    # lets delete the existing files or they will build up\n",
    "    for file in [f for f in listdir(path) if isfile(join(path, f))] :\n",
    "        os.remove(path+file)\n",
    "\n",
    "    # populate our map\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # get the date component from the datetime object\n",
    "        day=my_datetime.strftime(\"%Y-%m-%d\")\n",
    "        if day not in date_map :\n",
    "            date_map[day] = []\n",
    "        date_map[day].append(my_datetime.strftime(\"%H:%M:%S.%f\"))            \n",
    "\n",
    "    # itterate through all the days and print out the times with a random log message\n",
    "    for my_day in date_map.keys() :\n",
    "        filename=path+my_day+\".log\"    \n",
    "        # filename for the days events, named after the day \"2020-02-12.log\"\n",
    "        file_for_day = open(filename,\"w\")\n",
    "        for my_time in date_map[my_day] :\n",
    "            # write out the timestamp with a random log message\n",
    "            file_for_day.write(my_time + \" \" + random.choice(log_lines)+ \"\\n\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This creates log lines with follow an attribute=value pattern using different and no quotes\n",
    "\n",
    "Specify the minimum and maximum number of av pairs per log line\n",
    "\"\"\"\n",
    "def auto_extract_indexed_fields(sample_readings : int, max_days_ago : int,  min_values : int, max_values : int) : \n",
    "\n",
    "    # a list of variable names for us to pull from, complete with a type field\n",
    "    variable_names= [('stdev_kbps',float),('average_kbps',float), ('sum_kbps',int), ('label',str), ('name',str), ('group',str), ('value',int)]\n",
    "    # a list of string values for us to pull from when building events\n",
    "    labels = ['no_quotes',\"'single quotes'\",'\"double quotes\"']\n",
    "\n",
    "    # We don't want some n00b specifying more AV pairs than we have in our sample group or we run out!\n",
    "    if (max_values>len(variable_names)) :\n",
    "        max_values=len(variable_names)\n",
    "\n",
    "    # open our output file\n",
    "    indexed_log = open(data_directory+\"/auto_extract_indexed_fields/indexed.log\",\"w\")\n",
    "\n",
    "    # get a selection of date times\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "\n",
    "        # We don't want the same variable printed multiple times, this would result in multivalue fields   \n",
    "        # Copy the our list of possible AV pairs     \n",
    "        my_variables_names=variable_names.copy()\n",
    "        # shuffle that list so they occur in a random order\n",
    "        random.shuffle(my_variables_names)\n",
    "\n",
    "        # we need an message to return, we will put the time stamp at the front\n",
    "        message = my_datetime.strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "        # We will append a random number of AV pairs to the message\n",
    "        for i in range(0,random.randint(min_values,max_values)) :\n",
    "            # pop off the variable name and the type that we are going to use\n",
    "            (variable_name, variable_type) = my_variables_names.pop(1)\n",
    "            # write out the variable name\n",
    "            message = message + \" \" + variable_name + \"=\" \n",
    "            # depending the type select a value\n",
    "            if (variable_type == str) : \n",
    "                message = message + random.choice(labels)\n",
    "            elif (variable_type == float) :\n",
    "                message = message + str(random.random())\n",
    "            elif (variable_type == int) :\n",
    "                message = message + str(random.randint(0,9999))\n",
    "\n",
    "        # write out the log name\n",
    "        indexed_log.write(message+\"\\n\")\n",
    "\n",
    "    # close the file\n",
    "    indexed_log.close()\n",
    "\n",
    "\"\"\"\n",
    "This script generates a data set for importing into directly into splunk. We have create sourcetype, source, host, index and then use INGEST_EVAL + REGEX to extract the fields and copy them into the relevant fields. \n",
    "\n",
    "The format aims to replicated the output of the following splunk search:\n",
    "\"\"\"\n",
    "def load_into_indexes(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # for this demo we needs some target indexes, sourcestypes, sources and hosts\n",
    "\n",
    "    # these indexes have been created in indexes.conf\n",
    "    indexes=['ingest_eval_examples_1', 'ingest_eval_examples_2']\n",
    "    # sourcetypes should have only one date format, lets match these together, these don't need to be defined in props.conf as the timestamp is written out in EPOCH\n",
    "    sourcetypes=[('load_into_indexes:fruit', \"%c\"), ('load_into_indexes:beef', \"%Y-%m-%d %H:%M:%S\"), ('load_into_indexes:fish', \"%H:%M:%S %y-%m-%d\"), ('load_into_indexes:chicken', \"%d %a %Y %H:%M:%S\")]\n",
    "    # a selection of values for source\n",
    "    sources=['load_into_indexes:sea', 'load_into_indexes:ground', 'load_into_indexes:sky', 'load_into_indexes:tree']\n",
    "    # a selection of values for host\n",
    "    hosts=['load_into_indexes:farm_shop', 'load_into_indexes:online', 'load_into_indexes:super_market', 'load_into_indexes:market']\n",
    "    # we also need something to separate the data\n",
    "    sep=\"%%%\"\n",
    "\n",
    "    # open the output log file to write data too\n",
    "    import_events = open(data_directory+'/load_into_indexes/encoded_splunk_events.csv',\"w\")\n",
    "\n",
    "    # fake the header row\n",
    "    import_events.write('\"_raw\\n\"')\n",
    "\n",
    "    # get a selection of date times\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # pick a sourcetype adn \n",
    "        (sourcetype, datetime_format) = random.choice(sourcetypes)\n",
    "        # get the epoch numeric value for the datetime\n",
    "        time=str(my_datetime.timestamp())\n",
    "        # pick a random host, source and index for the log line\n",
    "        host=random.choice(hosts)\n",
    "        source=random.choice(sources)\n",
    "        index=random.choice(indexes)\n",
    "        # generate the log line prefixed by the timestamp\n",
    "        raw=my_datetime.strftime(datetime_format)+\" \"+random.choice(log_lines)\n",
    "        # write out the line to be written into the import file\n",
    "        import_events.write('\"' + time + sep + index + sep + host + sep + source + sep + sourcetype + sep + raw + '^^^END^^^\"\\n')\n",
    "\n",
    "    # write to the output file\n",
    "    import_events.close()\n",
    "\n",
    "\"\"\" This function generates a text file with user names and passwords so we can demonstrate how to mask data\n",
    "\"\"\"\n",
    "# a selection of comedy passwords, for more bad passwords I recommend the excellent https://bad.pw/\n",
    "terrible_passwords = [\"password\", \"p@$$w0rd\", \"qwerty123\", \"aaaaa\", \"admin\", \"0000\", \"letmein\", \"onetimepassword\", \"assword\", \"abc123\", \"123456\", \"password1\", \"iloveyou\", \"trustno1\", \"iambatman\"]\n",
    "email_addresses = [\"admin@ghcq.com\", \"007@mi5.gov.uk\", \"putin@gru.ru\", \"director@cia.us.gov\", \"julian@wikileaks.com\", \"jbourne@ucia.gov\"]\n",
    "\n",
    "def mask_and_clone(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    joke_security=open(data_directory+\"/mask_and_clone/insecurity.log\", \"w\")\n",
    "\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        joke_security.write(my_datetime.strftime(\"%H:%M:%S %y-%m-%d\") + \" my email_address=\" + random.choice(email_addresses) + ' and my terrible password=\"' + random.choice(terrible_passwords) + '\"\\n')\n",
    "   \n",
    "    joke_security.close()\n",
    "\n",
    "\n",
    "def mask_data_and_map(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    joke_security=open(data_directory+\"/mask_data_and_map/insecurity.log\", \"w\")\n",
    "\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        joke_security.write(my_datetime.strftime(\"%H:%M:%S %y-%m-%d\") + \" my email_address=\" + random.choice(email_addresses) + ' and my terrible password=\"' + random.choice(terrible_passwords) + '\"\\n')\n",
    "   \n",
    "    joke_security.close()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script generates a csv with 'useless' columns that we don't want to add into tsidx because they will bloat the size of the bucket.\n",
    "\n",
    "We use pandas to build the CSV file, set headers etc\n",
    "\"\"\"\n",
    "def drop_indexed_fields(sample_readings : int, max_days_ago : int)  :\n",
    "    # create a pandas with some column headings describing the contents\n",
    "    useless_columns=pandas.DataFrame(columns=['time','primary_key', 'primary_value', 'repeated_field', 'random_nonsense', 'long_payload'])\n",
    "\n",
    "    # Create rows and assign values to the columns\n",
    "    for my_date in get_dates(sample_readings, max_days_ago) :\n",
    "        useless_columns=useless_columns.append({'time': my_date, 'primary_key': hex(random.randint(0,pow(2,16))), 'primary_value': random.randint(0,999999), 'repeated_field': \"same silly value\", 'random_nonsense' : uuid.uuid4(), 'long_payload' : random.choice(log_lines)}, ignore_index=True)\n",
    "\n",
    "    # write out the CSV file\n",
    "    useless_columns.to_csv(data_directory+'/drop_indexed_fields/useless_columns.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "\"\"\"\n",
    "This generates a simple log file for split forwarding, it has no interesting attributes\n",
    "\"\"\"\n",
    "def split_forwarding(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # create out output file\n",
    "    mutliplexed_datetime_formats = open(data_directory+\"/split_forwarding/simple_events.log\",\"w\")\n",
    "\n",
    "    # iterate through the list of date timesn and write out to disk\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # get the date as a string\n",
    "        time=my_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # pick a random log line to use\n",
    "        message=random.choice(log_lines)\n",
    "        # write out the log file\n",
    "        mutliplexed_datetime_formats.write(time+\" \"+message+\"\\n\")\n",
    "\n",
    "    # close and flush the file\n",
    "    mutliplexed_datetime_formats.close()\n",
    "\n",
    "\"\"\"\n",
    "In splunk 8.1 we introduced thread name and ids to splunk logs, this function generates events in the splunkd format to test both\n",
    "formatting styles so that we are able read both cleanly\n",
    "\"\"\"\n",
    "def enrich_splunkd_component_level_thread(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # 09-21-2020 09:51:06.241 +0000 INFO  TailReader [10099 batchreader1] - Starting batchreader1 thread\n",
    "    # 09-21-2020 09:50:57.229 +0000 INFO  CMSlave [9659 MainThread] - starting heartbeat thread\n",
    "    # 09-21-2020 09:50:57.229 +0000 INFO  CMSlave - starting heartbeat thread\n",
    "\n",
    "    components = [\"ApplicationLicense\", \"ApplicationManager\", \"ApplicationUpdater\", \"BucketMover\", \"BucketReplicator\", \"BundleJob\", \"BundlesSetup\", \"CMBucketId\", \"CMBundleMgr\", \"CMHeartbeatThread\", \"CMRepJob\", \"CMReplicationRegistry\", \"CMServiceThread\", \"CMSlave\", \"CacheManager\", \"CascadingReplicationManager\", \"CertStorageProvider\", \"ClientSessionsManager\", \"ClusterBundleValidator\", \"ClusterSlaveConfigReloader\", \"ClusterSlaveControlHandler\", \"ClusteringMgr\", \"DC:DeploymentClient\", \"DSManager\", \"DS_DC_Common\", \"DatabaseDirectoryManager\", \"DeploymentServer\", \"ExecProcessor\", \"HotBucketRoller\", \"HotDBManager\", \"IndexProcessor\", \"IndexWriter\", \"IndexerIf\", \"IndexerInit\", \"IndexerService\", \"IntrospectionGenerator:disk_objects\", \"IntrospectionGenerator:resource_usage\", \"KVStoreBackupRestore\", \"KeyManagerSearchPeers\", \"LMConfig\", \"LMSlaveInfo\", \"LMStackMgr\", \"LMTracker\", \"LicenseMgr\", \"MPool\", \"MetricAlertManager\", \"MetricSchemaProcessor\", \"MetricsProcessor\", \"ModularInputs\", \"MultiFactorAuthManager\", \"NoahHeartbeat\", \"PipeFlusher\", \"PipelineComponent\", \"ProxyConfig\", \"PubSubSvr\", \"RemoteQueueInputProcessor\", \"Rsa2FA\", \"S2SFileReceiver\", \"SHClusterMgr\", \"ScheduledViewsReaper\", \"ServerConfig\", \"ServerRoles\", \"ShutdownHandler\", \"SpecFiles\", \"StorageInterface\", \"StreamingBucketBuilder\", \"TailReader\", \"TailingProcessor\", \"TcpInputConfig\", \"TcpInputProc\", \"TcpOutputProc\", \"TelemetryHandler\", \"UDPInputProcessor\", \"UiHttpListener\", \"WatchedFile\", \"WorkloadManager\" ]\n",
    "\n",
    "    log_levels = ['INFO', 'WARN', 'ERROR', 'DEBUG']\n",
    "\n",
    "    roles = ['sh-i-','shc-i-','idx-i-','c0m1-i-','idm-i-']\n",
    "\n",
    "    threads=[\"All_CMExecutorsShutdownThread\", \"AppLicenseThread\", \"AuditSearchExecutor\", \"BundleExecutorWorker-0\", \"CMExecutorWorker-0\", \"CMExecutorWorker-1\", \"CMExecutorWorker-10\", \"CMExecutorWorker-2\", \"CMExecutorWorker-3\", \"CMExecutorWorker-4\", \"CMExecutorWorker-5\", \"CMExecutorWorker-6\", \"CMExecutorWorker-7\", \"CMExecutorWorker-8\", \"CMExecutorWorker-9\", \"CMHealthManager\", \"CMHeartbeatThread\", \"CMNotifyThread\", \"CMSlaveShutdownThread\", \"CMSynchronousExecutorWorker-0\", \"CMUploadReplicatedBucketThread\", \"CallbackRunnerThread\", \"DispatchReaper\", \"ExecProcessorSchedulerThread\", \"FilesystemOpExecutorWorker-0\", \"FilesystemOpExecutorWorker-1\", \"FilesystemOpExecutorWorker-2\", \"FilesystemOpExecutorWorker-3\", \"FilesystemOpExecutorWorker-4\", \"FilesystemOpExecutorWorker-5\", \"FilesystemOpExecutorWorker-6\", \"FilesystemOpExecutorWorker-7\", \"FlusherThread\", \"HTTPDispatch\", \"HttpDedicatedIoThread-1\", \"HttpDedicatedIoThread-3\", \"HttpDedicatedIoThread-4\", \"IndexInitExecutorWorker-0\", \"IndexInitExecutorWorker-1\", \"IndexerService\", \"IndexerTPoolWorker-0\", \"IndexerTPoolWorker-1\", \"KVStoreBackupThread\", \"Killa\", \"MainTailingThread\", \"MainThread\", \"ReplicationDataReceiverThread\", \"SchedulerThread\", \"Shutdown\", \"SplunkdSpecificInitThread\", \"TcpChannelThread\", \"TcpListener\", \"TcpOutEloop\", \"WebuiStartup\", \"batchreader0\", \"batchreader1\", \"cachemanagerDownloadExecutorWorker-0\", \"cachemanagerDownloadExecutorWorker-1\", \"cachemanagerDownloadExecutorWorker-2\", \"cachemanagerDownloadExecutorWorker-3\", \"cachemanagerDownloadExecutorWorker-4\", \"cachemanagerDownloadExecutorWorker-5\", \"cachemanagerDownloadExecutorWorker-6\", \"cachemanagerDownloadExecutorWorker-7\", \"cachemanagerDownloadExecutorWorker-8\", \"cachemanagerUploadExecutorWorker-0\", \"cachemanagerUploadExecutorWorker-1\", \"cachemanagerUploadExecutorWorker-10\", \"cachemanagerUploadExecutorWorker-2\", \"cachemanagerUploadExecutorWorker-3\", \"cachemanagerUploadExecutorWorker-4\", \"cachemanagerUploadExecutorWorker-5\", \"cachemanagerUploadExecutorWorker-6\", \"cachemanagerUploadExecutorWorker-7\", \"cachemanagerUploadExecutorWorker-8\", \"cachemanagerUploadExecutorWorker-9\", \"exec_1\", \"indexerPipe_0\", \"indexerPipe_1\", \"journal-compress\", \"remotequeueinput_0\", \"remotequeueinput_1\", \"tailreader0\", \"tailreader1\", \"tcp_0\", \"tcp_1\", \"typing_0\", \"typing_1\"]\n",
    "\n",
    "    enrich_splunkd = open(data_directory+'/enrich_splunkd_component_level_thread/multi_log_formats.log','w')\n",
    "\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        time=my_datetime.strftime(\"%d-%m-%Y %H:%M:%S.%f\")[0:23]\n",
    "        log_level=random.choice(log_levels)\n",
    "        component=random.choice(components)\n",
    "        thread_info=\"[\"+str(random.randrange(10,9999))+\" \"+random.choice(threads)+\"]\"\n",
    "        enrich_splunkd.write(time+\" +0000 \"+log_level+\"  \"+component+\" \"+random.choice([\"\",thread_info])+\" - \"+random.choice(log_lines)+'\\n')\n",
    "\n",
    "    enrich_splunkd.close()\n",
    "\n",
    "# generate some sample data for enriching splunkd_access.log\n",
    "def enrich_splunkd_access_log(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # example log line\n",
    "    # 127.0.0.1 - splunk-system-user [22/Sep/2020:09:51:03.999 +0000] \"POST \"POST /services/admin/cacheman/bid|_audit~418~35EAC499-A711-4E8C-8E33-5CB78009D53A|/close?output_mode=json&sid=remote_sh-i-066cb06a2a60e5d07.blah.splunkcloud.com_subsearch_1600768261.138727_BDC68508-A6EF-4FC4-BE64-487E7AB8EB1E_1600768262.1&miss_ms=0 HTTP/1.1\" 200 1976 - - - 0ms\n",
    "\n",
    "    ips = [ \"10.0.\"+str(random.randrange(1,254))+\".\"+str(random.randrange(1,254)) for i in range(0,10) ] + 5*[\"127.0.0.1\"]\n",
    "    user = [\"splunk-system-user\", \"internal_monitoring\", \"admin\", \"bob.smith@thesmiths.com\", \"tony@stark.com\", \"hulk@greenmachine.com\"]\n",
    "\n",
    "    sample = open(data_directory+\"/enrich_splunkd_access_log/splunkd_access.log.sample\",'r').readlines()\n",
    "    enrich_splunkd_access_log = open(data_directory+\"/enrich_splunkd_access_log/enrich_splunkd_access.log\",'w')\n",
    "\n",
    "    for i in get_dates(sample_readings,max_days_ago) :\n",
    "        log_line = random.choice(sample)\n",
    "        new_line=log_line.replace(\"$date$\", i.strftime(\"%d/%b/%Y:%H:%M:%S.%f\")[0:24]+random.choice([\" +0000\",\" -0000\",\" -0800\",\" +0600\",\" -1200\",\" +1100\",\" -0030\",\" +0145\"])).replace(\"$user$\",random.choice(user)).replace(\"$ip$\",random.choice(ips))\n",
    "        enrich_splunkd_access_log.write(new_line)\n",
    "\n",
    "    enrich_splunkd_access_log.close()\n",
    "\n",
    "\n",
    "def shard_data_with_splitbyindexkeys(sample_readings : int, max_days_ago : int, number_of_hosts : int) :\n",
    "\n",
    "    # we will emulate splunk cloud customer names \n",
    "\n",
    "    # a selection of roles to compose splunk instance names from\n",
    "    roles = [\"shc-i-\",\"idx-i-\", \"c0m1-i-\",\"sh-i-\", \"idm-i-\"]\n",
    "    # create our list of domain names\n",
    "    domains = ['.splunkcloud.com','.stg.splunkcloud.com']\n",
    "    # create a list of dummy customer names for the \"stacks\"\n",
    "    companies_names = ['Brawndo','Parallax','Megadodo','Adventureland','PierceandPierce','acme','umbrella','Initech','Rekall','SPECTRE','tyrell','The Daily Planet','Stark Industries','Cyberdyne Systems','Wayne Enterprises']\n",
    "\n",
    "    # create host names and roles, using the dummy names provided\n",
    "    hosts=[]\n",
    "    for i in range(0,number_of_hosts) :\n",
    "        # fake the amazon instance id\n",
    "        instance = ''.join(random.choice('0123456789abcdef') for n in range(17))\n",
    "        # pick a random role\n",
    "        role= random.choice(roles)\n",
    "        # pick a random domain\n",
    "        domain=random.choice(domains)\n",
    "        # convert the company name to a stack name\n",
    "        company=random.choice(companies_names).lower().replace(' ','')\n",
    "        # add our fake host to the list of hosts\n",
    "        hosts.append(role+instance+'.'+company+domain)\n",
    "\n",
    "    # load our frequency data\n",
    "    df = pandas.read_csv(data_directory+\"/shard_data_with_splitbyindexkeys/splunkd_frequency_analysis.csv\")\n",
    "\n",
    "    # sort by the count (frequency), smallest to largest\n",
    "    sorted_df = df.sort_values(\"count\")\n",
    "\n",
    "    # get our total search space, i.e. get the number of events found\n",
    "    total=df[\"count\"].sum()\n",
    "\n",
    "    # Add a rolling sum to use as our index\n",
    "    sorted_df[\"rolling_sum\"] = sorted_df.expanding(2).sum()\n",
    "\n",
    "    # we also need something to separate the data\n",
    "    sep=\"%%%\"\n",
    "\n",
    "    # open the output log line\n",
    "    shard_data_with_splitbyindexkeys = open(data_directory+\"/shard_data_with_splitbyindexkeys/shard_splunkd.log\",\"w\")\n",
    "\n",
    "    # place holders for the results for my_component and source\n",
    "    my_component = \"\"\n",
    "    my_source = \"\"\n",
    "\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        for (index,row) in sorted_df.iterrows() :\n",
    "            if row.rolling_sum >= random .randrange(1,total) :\n",
    "                my_component = row.component\n",
    "                my_source = row.source\n",
    "                break\n",
    "\n",
    "        # get the epoch numeric value for the datetime\n",
    "        time=str(my_datetime.timestamp())[0:20]\n",
    "        # pick a random host, source and index for the log line\n",
    "        host=random.choice(hosts)\n",
    "        # generate the log line prefixed by the timestamp\n",
    "        raw=my_datetime.strftime(\"%d-%m-%Y %H:%M:%S.%f\")+\" +0000 INFO  \" + my_component + \" - \" + random.choice(log_lines)\n",
    "        # write out the line to be written into the import file\n",
    "        shard_data_with_splitbyindexkeys.write('\"' + time + sep + \"shard_data_with_splitbyindexkeys\" + sep + host + sep + my_source + sep + \"splunkd\" + sep + raw + '^^^END^^^\"\\n')\n",
    "\n",
    "    # finished writing out the file\n",
    "    shard_data_with_splitbyindexkeys.close()\n",
    "\n",
    "def name_clash_zoom_data(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    user_type = [\"Basic\", \"Basic|Webinar500\", \"Licensed\", \"Licensed|Large500\", \"Licensed|Webinar1000\", \"Licensed|Webinar10000|Large500\", \"Licensed|Webinar1000|Large500\", \"Licensed|Webinar3000\", \"Licensed|Webinar3000|Large500\", \"Licensed|Webinar500\", \"Licensed|Webinar5000\", \"Licensed|Webinar5000|Large500\", \"Licensed|Webinar500|Large500\", \"On-Prem\", \"On-Prem|Webinar500\", \"Unknown\", \"Unknown|Webinar500|Large500\"]\n",
    "\n",
    "    name_clash=open(data_directory+\"/name_clash/naughty_zoom_host.log\", \"w\")\n",
    "\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "\n",
    "        meeting_duration=datetime.timedelta(hours=random.choice(range(1,3)), minutes=random.choice(range(-30,30)))\n",
    "\n",
    "        my_json = { \n",
    "            \"host\" : random.choice(email_addresses),\n",
    "            \"uuid\" : ''.join(random.choice(string.ascii_letters+\"+=/\") for n in range(22))+'==',\n",
    "            \"id\": random.randrange(100000000,99999999999),\n",
    "            \"topic\" : random.choice(log_lines),\n",
    "            \"email\" : random.choice(email_addresses),\n",
    "            \"user_type\" : random.choice(user_type),\n",
    "            \"start_time\" : my_datetime.strftime(\"%Y-%m-%dT%T\"),\n",
    "            \"end_time\" : (my_datetime+meeting_duration).strftime(\"%Y-%m-%dT%T\"),\n",
    "            \"duration\" : str(meeting_duration),\n",
    "            \"participants\": round(random.triangular(2,50,0.1)),\n",
    "            \"has_pstn\" : random.choice([True, False]),\n",
    "            \"has_voip\" : random.choice([True, False]),\n",
    "            \"has_3rd_party_audio\" : random.choice([True, False]),\n",
    "            \"has_video\" : random.choice([True, False]),\n",
    "            \"has_screen_share\" : random.choice([True, False]),\n",
    "            \"has_recording\" : random.choice([True, False]),\n",
    "            \"has_sip\" : random.choice([True, False])\n",
    "                }\n",
    "\n",
    "        name_clash.write(json.dumps(my_json)+\"\\n\")\n",
    "    name_clash.close()\n",
    "\n",
    "def json_logs(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # {\"log\":\"I1026 03:58:37.356178 1 binarylog.go:274] rpc: flushed binary log to \\\"\\\"\\n\",\"stream\":\"stderr\",\"time\":\"2020-10-26T03:58:37.356327831Z\"}\n",
    "    # {\"log\":\"Log line is here\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-01T11:11:11.111111111Z\"}\n",
    "\n",
    "    name_clash=open(data_directory+\"/json_docker/json_docker.log\", \"w\")\n",
    "\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        log = random.choice(log_lines)\n",
    "        time = my_datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")+str(random.randrange(0,999999999))+\"Z\"\n",
    "        stream = random.choice([\"stdout\",\"stderr\"])\n",
    "        json = '{\"log\":\"'+log+'\",\"stream\":\"'+stream+'\",\"time\":\"'+time+'\"}'\n",
    "        name_clash.write(json+\"\\n\")\n",
    "\n",
    "    name_clash.close()\n",
    "\n",
    "# this function returns things that look like IP addresses, but frequently aren't, this is to demonstrate how rejection of ip addresses work when they are embedded \n",
    "# in other messages\n",
    "def get_ip_address() :\n",
    "    \n",
    "    # lets some times have ips that occur a lot\n",
    "    if random.choice(range(100)) == 0 :\n",
    "        return random.choice([\"1.2.3.4\",\"8.8.8.8\",\"10.0.0.1\"])\n",
    "    else :\n",
    "        # the use of traingular means that most ip address occurs with segments under 255, but occasionally ip address addresses are generated \n",
    "        return \".\".join([ str(round(random.triangular(0,100,800))) for i in range(random.choice([4 for i in range(20)]+[3,5,6]))])\n",
    "\n",
    "# Create some messages that randomly contain things that look like ip addresses to demonstrate how regex can be used to lift ip addresses\n",
    "def find_ip_addresses(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # open our file\n",
    "    may_contain_ips=open(data_directory+\"/find_ip_addresses/may_contain_ips.log\", \"w\")\n",
    "\n",
    "    # lets prefix the ip adresses with some junk\n",
    "    prefix = ['ip=','dest=','http://','source=','from=','to=']\n",
    "\n",
    "    # for all our sample dates\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        message=\"\"\n",
    "        # lets have up to 5 ip addresses per event\n",
    "        for i in range(random.randrange(1,5)) :\n",
    "            ip=get_ip_address()\n",
    "            message=message+\" \"+random.choice(prefix)+ip \n",
    "        raw=my_datetime.strftime(\"%d-%m-%Y %H:%M:%S\")+message+\" \"+(\" \".join([ str(round(random.triangular(0,100,800))) for i in range(random.choice([random.randint(4,20) for i in range(20)]+[3,5,6]))]))\n",
    "        may_contain_ips.write(raw+' \\n')\n",
    "\n",
    "    # close our file\n",
    "    may_contain_ips.close()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "conflicting_datetime_formats(100,7)\n",
    "drop_indexed_fields(100,7)\n",
    "compound_datetimes(500,7)\n",
    "auto_extract_indexed_fields(100,7,2,5)\n",
    "load_into_indexes(1000,7)\n",
    "mask_and_clone(1000,7)\n",
    "load_into_indexes(100,5)\n",
    "enrich_splunkd_component_level_thread(100,7)\n",
    "enrich_splunkd_access_log(1000,7)\n",
    "shard_data_with_splitbyindexkeys(100,2,7)\n",
    "mask_data_and_map(100,7)\n",
    "mask_and_clone(100,7)\n",
    "# name_clash_zoom_data(100,6)\n",
    "json_logs(100,5)\n",
    "find_ip_addresses(1000,300)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}